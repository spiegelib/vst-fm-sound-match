{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sound Match Deep Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we perform sound matching of the evaluation target set using the trained deep learning models and save the resulting audio files to disk for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spiegelib as spgl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all saved models\n",
    "mlp = spgl.estimator.TFEstimatorBase.load('./saved_models/simple_fm_mlp.h5')\n",
    "lstm = spgl.estimator.TFEstimatorBase.load('./saved_models/simple_fm_lstm.h5')\n",
    "bi_lstm = spgl.estimator.TFEstimatorBase.load('./saved_models/simple_fm_bi_lstm.h5')\n",
    "cnn = spgl.estimator.TFEstimatorBase.load('./saved_models/simple_fm_cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load synth with overriden params\n",
    "synth = spgl.synth.SynthVST(\"/Library/Audio/Plug-Ins/VST/Dexed.vst\",\n",
    "                               note_length_secs=1.0, render_length_secs=1.0)\n",
    "synth.load_state(\"./synth_params/dexed_simple_fm.json\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup all the feature extractors to provide the correct input data for each model based on how it was trained. Also use the same data scalers that were setup when initially creating each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP feature extractor with a modifying function that flattens the time slice arrays at the end of the feature\n",
    "# extraction pipeline\n",
    "mlp_extractor = spgl.features.MFCC(num_mfccs=13, time_major=True, hop_size=1024, scale=True)\n",
    "mlp_extractor.load_scaler('./data_simple_fm_mfcc/data_scaler.pkl')\n",
    "mlp_extractor.add_modifier(lambda data : data.flatten(), type='output')\n",
    "\n",
    "# LSTM & LSTM++ feature extractor -- time series of MFCC frames\n",
    "lstm_extractor = spgl.features.MFCC(num_mfccs=13, time_major=True, hop_size=1024, scale=True)\n",
    "lstm_extractor.load_scaler('./data_simple_fm_mfcc/data_scaler.pkl')\n",
    "\n",
    "# CNN feature extractor uses magnitude output from STFT and then modifies the output array into a 3D array for the\n",
    "# 2D convolutional network becuase it is expecting an image with a single channel (ie grayscale).\n",
    "cnn_extractor = spgl.features.STFT(output='magnitude', fft_size=512, hop_size=256, time_major=True, scale=True)\n",
    "cnn_extractor.load_scaler('./data_simple_fm_stft/data_scaler.pkl')\n",
    "cnn_extractor.add_modifier(lambda data : data.reshape(data.shape[0], data.shape[1], 1), type='output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SoundMatch is a class designed to help run sound matches for a synthesizer and a specific estimator type. Each SoundMatch object requires a synthesizer to use to generate sounds, an estimator object, and optionally an audio feature extractor object. If an audio feature object is provided, that will be used to extract features from incoming audio prior to running estimation. This is required for these deep learning models, but some estimators can handle raw audio, such as the genetic estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_matcher = spgl.SoundMatch(synth, mlp, mlp_extractor)\n",
    "lstm_matcher = spgl.SoundMatch(synth, lstm, lstm_extractor)\n",
    "bi_lstm_matcher = spgl.SoundMatch(synth, bi_lstm, lstm_extractor)\n",
    "cnn_matcher = spgl.SoundMatch(synth, cnn, cnn_extractor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the folder of evaluation audio samples and perform sound matching on each one with each estimation model. AudioBuffer.load_folder performs a natural sort based on the file names of the audio contained in the specified folder, so we can save each prediction with a corresponding integer number and be assured that the ordering will match up when we get to evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = spgl.AudioBuffer.load_folder('./evaluation/audio')\n",
    "\n",
    "for i in range(len(targets)):\n",
    "    audio = mlp_matcher.match(targets[i])\n",
    "    audio.save('./evaluation/mlp/mlp_prediction_%s.wav' % i)\n",
    "\n",
    "    audio = lstm_matcher.match(targets[i])\n",
    "    audio.save('./evaluation/lstm/lstm_prediction_%s.wav' % i)\n",
    "\n",
    "    audio = bi_lstm_matcher.match(targets[i])\n",
    "    audio.save('./evaluation/bi_lstm/bi_lstm_prediction_%s.wav' % i)\n",
    "\n",
    "    audio = cnn_matcher.match(targets[i])\n",
    "    audio.save('./evaluation/cnn/cnn_prediction_%s.wav' % i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
